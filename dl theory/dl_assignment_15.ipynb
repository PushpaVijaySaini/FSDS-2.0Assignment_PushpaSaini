{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "1. Deep Learning.\n",
        "\n",
        "a. Build a DNN with five hidden layers of 100 neurons each, He initialization, and the ELU activation function.\n",
        "\n",
        "b. Using Adam optimization and early stopping, try training it on MNIST but only on digits 0 to 4, as we will use transfer learning for digits 5 to 9 in the next exercise. You will need a softmax output layer with five neurons, and as always make sure to save checkpoints at regular intervals and save the final model so you can reuse it later.\n",
        "\n",
        "c. Tune the hyperparameters using cross-validation and see what precision you can achieve.\n",
        "\n",
        "d. Now try adding Batch Normalization and compare the learning curves: is it converging faster than before? Does it produce a better model?\n",
        "\n",
        "e. Is the model overfitting the training set? Try adding dropout to every layer and try again. Does it help?\n",
        "\n",
        "Ans: a. Build a DNN with five hidden layers of 100 neurons each, He initialization, and the ELU activation function.\n",
        "\n",
        "import tensorflow as tf\n",
        "from functools import partial\n",
        "# Define the number of inputs and outputs\n",
        "n_inputs = 28 * 28 # MNIST\n",
        "n_outputs = 10\n",
        "# Define the number of neurons in each hidden layer\n",
        "n_hidden1 = 100\n",
        "n_hidden2 = 100\n",
        "n_hidden3 = 100\n",
        "n_hidden4 = 100\n",
        "n_hidden5 = 100\n",
        "# Define the initializer and activation function to use\n",
        "he_init = tf.keras.initializers.VarianceScaling(scale=2., mode='fan_avg', distribution='uniform')\n",
        "elu_activation = tf.keras.activations.elu\n",
        "# Create the model using the Sequential API\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=[28, 28]), # Flatten the input data\n",
        "    tf.keras.layers.Dense(n_hidden1, activation=elu_activation, kernel_initializer=he_init), # First hidden layer\n",
        "    tf.keras.layers.Dense(n_hidden2, activation=elu_activation, kernel_initializer=he_init), # Second hidden layer\n",
        "    tf.keras.layers.Dense(n_hidden3, activation=elu_activation, kernel_initializer=he_init), # Third hidden layer\n",
        "    tf.keras.layers.Dense(n_hidden4, activation=elu_activation, kernel_initializer=he_init), # Fourth hidden layer\n",
        "    tf.keras.layers.Dense(n_hidden5, activation=elu_activation, kernel_initializer=he_init), # Fifth hidden layer\n",
        "    tf.keras.layers.Dense(n_outputs) # Output layer\n",
        "])\n",
        "b. Using Adam optimization and early stopping, try training it on MNIST but only on digits 0 to 4, as we will use transfer learning for digits 5 to 9 in the next exercise. You will need a softmax output layer with five neurons, and as always make sure to save checkpoints at regular intervals and save the final model so you can reuse it later.\n",
        "\n",
        "Ans:\n",
        "\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "# Load MNIST dataset\n",
        "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.mnist.load_data()\n",
        "# Scale the data\n",
        "X_train_full = X_train_full / 255.0\n",
        "X_test = X_test / 255.0\n",
        "# Create a validation set and training set\n",
        "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
        "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
        "# Only keep digits 0 to 4 in the training and validation sets\n",
        "X_train = X_train[y_train < 5]\n",
        "y_train = y_train[y_train < 5]\n",
        "X_valid = X_valid[y_valid < 5]\n",
        "y_valid = y_valid[y_valid < 5]\n",
        "# Build the model\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Flatten(input_shape=[28, 28]),\n",
        "    keras.layers.Dense(300, activation=\"relu\"),\n",
        "    keras.layers.Dense(100, activation=\"relu\"),\n",
        "    keras.layers.Dense(5, activation=\"softmax\")\n",
        "])\n",
        "# Compile the model\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "              optimizer=\"adam\",\n",
        "              metrics=[\"accuracy\"])\n",
        "# Define early stopping\n",
        "early_stopping_cb = keras.callbacks.EarlyStopping(patience=10,\n",
        "                                                  restore_best_weights=True)\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=30,\n",
        "                    validation_data=(X_valid, y_valid),\n",
        "                    callbacks=[early_stopping_cb])\n",
        "Epoch 1/30\n",
        "877/877 [==============================] - 6s 6ms/step - loss: 0.1940 - accuracy: 0.9386 - val_loss: 0.0424 - val_accuracy: 0.9859\n",
        "Epoch 2/30\n",
        "877/877 [==============================] - 3s 4ms/step - loss: 0.0374 - accuracy: 0.9893 - val_loss: 0.0356 - val_accuracy: 0.9867\n",
        "Epoch 3/30\n",
        "877/877 [==============================] - 3s 4ms/step - loss: 0.0218 - accuracy: 0.9930 - val_loss: 0.0346 - val_accuracy: 0.9891\n",
        "Epoch 4/30\n",
        "877/877 [==============================] - 4s 4ms/step - loss: 0.0141 - accuracy: 0.9958 - val_loss: 0.0381 - val_accuracy: 0.9883\n",
        "Epoch 5/30\n",
        "877/877 [==============================] - 3s 4ms/step - loss: 0.0122 - accuracy: 0.9962 - val_loss: 0.0345 - val_accuracy: 0.9898\n",
        "Epoch 6/30\n",
        "877/877 [==============================] - 4s 4ms/step - loss: 0.0086 - accuracy: 0.9973 - val_loss: 0.0272 - val_accuracy: 0.9918\n",
        "Epoch 7/30\n",
        "877/877 [==============================] - 3s 4ms/step - loss: 0.0083 - accuracy: 0.9971 - val_loss: 0.0312 - val_accuracy: 0.9918\n",
        "Epoch 8/30\n",
        "877/877 [==============================] - 4s 4ms/step - loss: 0.0034 - accuracy: 0.9992 - val_loss: 0.0277 - val_accuracy: 0.9941\n",
        "Epoch 9/30\n",
        "877/877 [==============================] - 3s 4ms/step - loss: 0.0062 - accuracy: 0.9982 - val_loss: 0.0251 - val_accuracy: 0.9937\n",
        "Epoch 10/30\n",
        "877/877 [==============================] - 3s 4ms/step - loss: 0.0055 - accuracy: 0.9983 - val_loss: 0.0398 - val_accuracy: 0.9906\n",
        "Epoch 11/30\n",
        "877/877 [==============================] - 3s 4ms/step - loss: 0.0083 - accuracy: 0.9978 - val_loss: 0.0324 - val_accuracy: 0.9922\n",
        "Epoch 12/30\n",
        "877/877 [==============================] - 4s 4ms/step - loss: 0.0047 - accuracy: 0.9983 - val_loss: 0.0278 - val_accuracy: 0.9930\n",
        "Epoch 13/30\n",
        "877/877 [==============================] - 3s 4ms/step - loss: 0.0042 - accuracy: 0.9987 - val_loss: 0.0393 - val_accuracy: 0.9898\n",
        "Epoch 14/30\n",
        "877/877 [==============================] - 3s 4ms/step - loss: 0.0051 - accuracy: 0.9983 - val_loss: 0.0398 - val_accuracy: 0.9918\n",
        "Epoch 15/30\n",
        "877/877 [==============================] - 3s 4ms/step - loss: 5.5724e-04 - accuracy: 1.0000 - val_loss: 0.0409 - val_accuracy: 0.9930\n",
        "Epoch 16/30\n",
        "877/877 [==============================] - 4s 4ms/step - loss: 9.6116e-05 - accuracy: 1.0000 - val_loss: 0.0396 - val_accuracy: 0.9930\n",
        "Epoch 17/30\n",
        "877/877 [==============================] - 3s 4ms/step - loss: 1.7608e-05 - accuracy: 1.0000 - val_loss: 0.0384 - val_accuracy: 0.9934\n",
        "Epoch 18/30\n",
        "877/877 [==============================] - 4s 4ms/step - loss: 9.7778e-06 - accuracy: 1.0000 - val_loss: 0.0385 - val_accuracy: 0.9934\n",
        "Epoch 19/30\n",
        "877/877 [==============================] - 3s 4ms/step - loss: 6.5843e-06 - accuracy: 1.0000 - val_loss: 0.0383 - val_accuracy: 0.9934\n",
        "# Save the final model\n",
        "model.save(\"my_mnist_model.h5\")\n",
        "c. Tune the hyperparameters using cross-validation and see what precision you can achieve.\n",
        "\n",
        "Ans:\n",
        "\n",
        "from tensorflow import keras\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import precision_score, make_scorer\n",
        "import numpy as np\n",
        "# Load MNIST dataset\n",
        "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.mnist.load_data()\n",
        "# Scale the data\n",
        "X_train_full = X_train_full / 255.0\n",
        "X_test = X_test / 255.0\n",
        "# Create a validation set and training set\n",
        "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
        "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
        "# Only keep digits 0 to 4 in the training and validation sets\n",
        "X_train = X_train[y_train < 5]\n",
        "y_train = y_train[y_train < 5]\n",
        "X_valid = X_valid[y_valid < 5]\n",
        "y_valid = y_valid[y_valid < 5]\n",
        "# Build a function to create the model\n",
        "def build_model(n_hidden=1, n_neurons=30, learning_rate=3e-3):\n",
        "    model = keras.models.Sequential()\n",
        "    model.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
        "    for layer in range(n_hidden):\n",
        "        model.add(keras.layers.Dense(n_neurons, activation=\"relu\"))\n",
        "    model.add(keras.layers.Dense(5, activation=\"softmax\"))\n",
        "    optimizer = keras.optimizers.Adam(lr=learning_rate)\n",
        "    model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n",
        "                  metrics=[\"accuracy\"])\n",
        "    return model\n",
        "# Create a KerasClassifier object\n",
        "keras_clf = keras.wrappers.scikit_learn.KerasClassifier(build_model)\n",
        "# Define the hyperparameters to search over\n",
        "param_grid = {\n",
        "    \"n_hidden\": [1, 2],\n",
        "    \"n_neurons\": [10, 30],\n",
        "    \"learning_rate\": [3e-4, 3e-3]\n",
        "}\n",
        "# Create a GridSearchCV object\n",
        "grid_search_cv = GridSearchCV(keras_clf, param_grid,\n",
        "                              cv=3,\n",
        "                              scoring=make_scorer(precision_score,\n",
        "                                                  average='weighted'))\n",
        "# Fit the GridSearchCV object to the data\n",
        "grid_search_cv.fit(X_train, y_train,\n",
        "                   validation_data=(X_valid, y_valid),\n",
        "                   callbacks=[keras.callbacks.EarlyStopping(patience=10)])\n",
        "585/585 [==============================] - 2s 3ms/step - loss: 0.9250 - accuracy: 0.7263 - val_loss: 0.2182 - val_accuracy: 0.9468\n",
        "D:\\mihir\\GitHub Projects\\iNeuron-Data-Science-Assignments\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
        "  warnings.warn('`model.predict_classes()` is deprecated and '\n",
        "585/585 [==============================] - 2s 3ms/step - loss: 0.8599 - accuracy: 0.7239 - val_loss: 0.2061 - val_accuracy: 0.9496\n",
        "D:\\mihir\\GitHub Projects\\iNeuron-Data-Science-Assignments\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
        "  warnings.warn('`model.predict_classes()` is deprecated and '\n",
        "585/585 [==============================] - 2s 4ms/step - loss: 0.9599 - accuracy: 0.7028 - val_loss: 0.2398 - val_accuracy: 0.9421\n",
        "D:\\mihir\\GitHub Projects\\iNeuron-Data-Science-Assignments\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
        "  warnings.warn('`model.predict_classes()` is deprecated and '\n",
        "585/585 [==============================] - 3s 4ms/step - loss: 0.6139 - accuracy: 0.8385 - val_loss: 0.1341 - val_accuracy: 0.9648\n",
        "D:\\mihir\\GitHub Projects\\iNeuron-Data-Science-Assignments\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
        "  warnings.warn('`model.predict_classes()` is deprecated and '\n",
        "585/585 [==============================] - 3s 4ms/step - loss: 0.6712 - accuracy: 0.8148 - val_loss: 0.1416 - val_accuracy: 0.9582\n",
        "D:\\mihir\\GitHub Projects\\iNeuron-Data-Science-Assignments\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
        "  warnings.warn('`model.predict_classes()` is deprecated and '\n",
        "585/585 [==============================] - 3s 4ms/step - loss: 0.7230 - accuracy: 0.7710 - val_loss: 0.1437 - val_accuracy: 0.9613\n",
        "D:\\mihir\\GitHub Projects\\iNeuron-Data-Science-Assignments\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
        "  warnings.warn('`model.predict_classes()` is deprecated and '\n",
        "585/585 [==============================] - 3s 4ms/step - loss: 0.9134 - accuracy: 0.7176 - val_loss: 0.1885 - val_accuracy: 0.9492\n",
        "D:\\mihir\\GitHub Projects\\iNeuron-Data-Science-Assignments\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
        "  warnings.warn('`model.predict_classes()` is deprecated and '\n",
        "585/585 [==============================] - 3s 4ms/step - loss: 1.1585 - accuracy: 0.5175 - val_loss: 0.3003 - val_accuracy: 0.9472\n",
        "D:\\mihir\\GitHub Projects\\iNeuron-Data-Science-Assignments\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
        "  warnings.warn('`model.predict_classes()` is deprecated and '\n",
        "585/585 [==============================] - 3s 4ms/step - loss: 1.1629 - accuracy: 0.5542 - val_loss: 0.2745 - val_accuracy: 0.9457\n",
        "D:\\mihir\\GitHub Projects\\iNeuron-Data-Science-Assignments\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
        "  warnings.warn('`model.predict_classes()` is deprecated and '\n",
        "585/585 [==============================] - 3s 4ms/step - loss: 0.7268 - accuracy: 0.7942 - val_loss: 0.1261 - val_accuracy: 0.9633\n",
        "D:\\mihir\\GitHub Projects\\iNeuron-Data-Science-Assignments\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
        "  warnings.warn('`model.predict_classes()` is deprecated and '\n",
        "585/585 [==============================] - 3s 4ms/step - loss: 0.7722 - accuracy: 0.7789 - val_loss: 0.1233 - val_accuracy: 0.9664\n",
        "D:\\mihir\\GitHub Projects\\iNeuron-Data-Science-Assignments\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
        "  warnings.warn('`model.predict_classes()` is deprecated and '\n",
        "585/585 [==============================] - 3s 4ms/step - loss: 0.6986 - accuracy: 0.7957 - val_loss: 0.1214 - val_accuracy: 0.9644\n",
        "D:\\mihir\\GitHub Projects\\iNeuron-Data-Science-Assignments\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
        "  warnings.warn('`model.predict_classes()` is deprecated and '\n",
        "585/585 [==============================] - 3s 4ms/step - loss: 0.3544 - accuracy: 0.8922 - val_loss: 0.1009 - val_accuracy: 0.9703\n",
        "D:\\mihir\\GitHub Projects\\iNeuron-Data-Science-Assignments\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
        "  warnings.warn('`model.predict_classes()` is deprecated and '\n",
        "585/585 [==============================] - 3s 4ms/step - loss: 0.3741 - accuracy: 0.8763 - val_loss: 0.1089 - val_accuracy: 0.9672\n",
        "D:\\mihir\\GitHub Projects\\iNeuron-Data-Science-Assignments\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
        "  warnings.warn('`model.predict_classes()` is deprecated and '\n",
        "585/585 [==============================] - 3s 4ms/step - loss: 0.3607 - accuracy: 0.8860 - val_loss: 0.0950 - val_accuracy: 0.9746\n",
        "D:\\mihir\\GitHub Projects\\iNeuron-Data-Science-Assignments\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
        "  warnings.warn('`model.predict_classes()` is deprecated and '\n",
        "585/585 [==============================] - 3s 4ms/step - loss: 0.2270 - accuracy: 0.9291 - val_loss: 0.0807 - val_accuracy: 0.9738\n",
        "D:\\mihir\\GitHub Projects\\iNeuron-Data-Science-Assignments\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
        "  warnings.warn('`model.predict_classes()` is deprecated and '\n",
        "585/585 [==============================] - 3s 4ms/step - loss: 0.2387 - accuracy: 0.9274 - val_loss: 0.0737 - val_accuracy: 0.9797\n",
        "D:\\mihir\\GitHub Projects\\iNeuron-Data-Science-Assignments\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
        "  warnings.warn('`model.predict_classes()` is deprecated and '\n",
        "585/585 [==============================] - 3s 4ms/step - loss: 0.2594 - accuracy: 0.9206 - val_loss: 0.0876 - val_accuracy: 0.9742\n",
        "D:\\mihir\\GitHub Projects\\iNeuron-Data-Science-Assignments\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
        "  warnings.warn('`model.predict_classes()` is deprecated and '\n",
        "585/585 [==============================] - 3s 4ms/step - loss: 0.4366 - accuracy: 0.8464 - val_loss: 0.0993 - val_accuracy: 0.9699\n",
        "D:\\mihir\\GitHub Projects\\iNeuron-Data-Science-Assignments\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
        "  warnings.warn('`model.predict_classes()` is deprecated and '\n",
        "585/585 [==============================] - 3s 4ms/step - loss: 0.3590 - accuracy: 0.9084 - val_loss: 0.0927 - val_accuracy: 0.9742\n",
        "D:\\mihir\\GitHub Projects\\iNeuron-Data-Science-Assignments\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
        "  warnings.warn('`model.predict_classes()` is deprecated and '\n",
        "585/585 [==============================] - 3s 4ms/step - loss: 0.4624 - accuracy: 0.8360 - val_loss: 0.1034 - val_accuracy: 0.9699\n",
        "D:\\mihir\\GitHub Projects\\iNeuron-Data-Science-Assignments\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
        "  warnings.warn('`model.predict_classes()` is deprecated and '\n",
        "585/585 [==============================] - 3s 4ms/step - loss: 0.2668 - accuracy: 0.9169 - val_loss: 0.0703 - val_accuracy: 0.9793\n",
        "D:\\mihir\\GitHub Projects\\iNeuron-Data-Science-Assignments\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
        "  warnings.warn('`model.predict_classes()` is deprecated and '\n",
        "585/585 [==============================] - 3s 4ms/step - loss: 0.2856 - accuracy: 0.9043 - val_loss: 0.0836 - val_accuracy: 0.9730\n",
        "D:\\mihir\\GitHub Projects\\iNeuron-Data-Science-Assignments\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
        "  warnings.warn('`model.predict_classes()` is deprecated and '\n",
        "585/585 [==============================] - 3s 5ms/step - loss: 0.2670 - accuracy: 0.9211 - val_loss: 0.0937 - val_accuracy: 0.9719\n",
        "D:\\mihir\\GitHub Projects\\iNeuron-Data-Science-Assignments\\venv\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
        "  warnings.warn('`model.predict_classes()` is deprecated and '\n",
        "877/877 [==============================] - 4s 4ms/step - loss: 0.2110 - accuracy: 0.9366 - val_loss: 0.0666 - val_accuracy: 0.9808\n",
        "GridSearchCV(cv=3,\n",
        "             estimator=<tensorflow.python.keras.wrappers.scikit_learn.KerasClassifier object at 0x0000021C70483CD0>,\n",
        "             param_grid={'learning_rate': [0.0003, 0.003], 'n_hidden': [1, 2],\n",
        "                         'n_neurons': [10, 30]},\n",
        "             scoring=make_scorer(precision_score, average=weighted))\n",
        "In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook.\n",
        "On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.\n",
        "# Print the best hyperparameters\n",
        "print(grid_search_cv.best_params_)\n",
        "{'learning_rate': 0.003, 'n_hidden': 1, 'n_neurons': 30}\n",
        "d. Now try adding Batch Normalization and compare the learning curves: is it converging faster than before? Does it produce a better model?\n",
        "\n",
        "Ans:\n",
        "\n",
        "# Build DNN with Batch Normalization\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Flatten(input_shape=[28, 28]),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
        "    keras.layers.BatchNormalization(),\n",
        "    keras.layers.Dense(5, activation=\"softmax\")\n",
        "])\n",
        "# Compile model\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "              optimizer=keras.optimizers.Adam(),\n",
        "              metrics=[\"accuracy\"])\n",
        "# Train model\n",
        "history_bn = model.fit(X_train, y_train, epochs=100, batch_size=32,\n",
        "                       validation_data=(X_valid, y_valid),\n",
        "                       callbacks=[early_stopping_cb])\n",
        "Epoch 1/100\n",
        "877/877 [==============================] - 9s 9ms/step - loss: 0.2128 - accuracy: 0.9302 - val_loss: 0.0711 - val_accuracy: 0.9797\n",
        "Epoch 2/100\n",
        "877/877 [==============================] - 7s 8ms/step - loss: 0.0583 - accuracy: 0.9830 - val_loss: 0.0493 - val_accuracy: 0.9836\n",
        "Epoch 3/100\n",
        "877/877 [==============================] - 7s 8ms/step - loss: 0.0436 - accuracy: 0.9865 - val_loss: 0.0463 - val_accuracy: 0.9875\n",
        "Epoch 4/100\n",
        "877/877 [==============================] - 7s 8ms/step - loss: 0.0312 - accuracy: 0.9903 - val_loss: 0.0502 - val_accuracy: 0.9832\n",
        "Epoch 5/100\n",
        "877/877 [==============================] - 7s 8ms/step - loss: 0.0282 - accuracy: 0.9902 - val_loss: 0.0314 - val_accuracy: 0.9902\n",
        "Epoch 6/100\n",
        "877/877 [==============================] - 7s 8ms/step - loss: 0.0261 - accuracy: 0.9917 - val_loss: 0.0375 - val_accuracy: 0.9879\n",
        "Epoch 7/100\n",
        "877/877 [==============================] - 7s 8ms/step - loss: 0.0232 - accuracy: 0.9926 - val_loss: 0.0333 - val_accuracy: 0.9891\n",
        "Epoch 8/100\n",
        "877/877 [==============================] - 7s 8ms/step - loss: 0.0202 - accuracy: 0.9928 - val_loss: 0.0368 - val_accuracy: 0.9875\n",
        "Epoch 9/100\n",
        "877/877 [==============================] - 7s 8ms/step - loss: 0.0181 - accuracy: 0.9939 - val_loss: 0.0509 - val_accuracy: 0.9863\n",
        "Epoch 10/100\n",
        "877/877 [==============================] - 7s 8ms/step - loss: 0.0157 - accuracy: 0.9945 - val_loss: 0.0467 - val_accuracy: 0.9871\n",
        "Epoch 11/100\n",
        "877/877 [==============================] - 7s 8ms/step - loss: 0.0150 - accuracy: 0.9952 - val_loss: 0.0494 - val_accuracy: 0.9871\n",
        "Epoch 12/100\n",
        "877/877 [==============================] - 7s 8ms/step - loss: 0.0122 - accuracy: 0.9967 - val_loss: 0.0382 - val_accuracy: 0.9879\n",
        "Epoch 13/100\n",
        "877/877 [==============================] - 7s 8ms/step - loss: 0.0150 - accuracy: 0.9951 - val_loss: 0.0438 - val_accuracy: 0.9887\n",
        "Epoch 14/100\n",
        "877/877 [==============================] - 7s 8ms/step - loss: 0.0104 - accuracy: 0.9968 - val_loss: 0.0394 - val_accuracy: 0.9902\n",
        "Epoch 15/100\n",
        "877/877 [==============================] - 8s 9ms/step - loss: 0.0152 - accuracy: 0.9954 - val_loss: 0.0434 - val_accuracy: 0.9910\n",
        "import matplotlib.pyplot as plt\n",
        "# Plot learning curves\n",
        "def plot_learning_curves(history, label=None):\n",
        "    plt.plot(history.history[\"loss\"], label=label + \" training\")\n",
        "    plt.plot(history.history[\"val_loss\"], label=label + \" validation\")\n",
        "    plt.legend()\n",
        "    plt.title(\"Learning curves\")\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.show()\n",
        "\n",
        "plot_learning_curves(history, label=\"Original\")\n",
        "plot_learning_curves(history_bn, label=\"Batch Normalization\")\n",
        "\n",
        "\n",
        "e. Is the model overfitting the training set? Try adding dropout to every layer and try again. Does it help?\n",
        "\n",
        "Ans:\n",
        "\n",
        "# Build DNN with Dropout\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Flatten(input_shape=[28, 28]),\n",
        "    keras.layers.Dropout(rate=0.2),\n",
        "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
        "    keras.layers.Dropout(rate=0.2),\n",
        "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
        "    keras.layers.Dropout(rate=0.2),\n",
        "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
        "    keras.layers.Dropout(rate=0.2),\n",
        "    keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"),\n",
        "    keras.layers.Dropout(rate=0.2),\n",
        "    keras.layers.Dense(5, activation=\"softmax\")\n",
        "])\n",
        "# Compile model\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "              optimizer=keras.optimizers.Adam(),\n",
        "              metrics=[\"accuracy\"])\n",
        "# Train model\n",
        "history_dropout = model.fit(X_train, y_train, epochs=100, batch_size=32,\n",
        "                            validation_data=(X_valid, y_valid),\n",
        "                            callbacks=[early_stopping_cb])\n",
        "Epoch 1/100\n",
        "877/877 [==============================] - 4s 4ms/step - loss: 0.3365 - accuracy: 0.8870 - val_loss: 0.0823 - val_accuracy: 0.9742\n",
        "Epoch 2/100\n",
        "877/877 [==============================] - 4s 4ms/step - loss: 0.1282 - accuracy: 0.9619 - val_loss: 0.0628 - val_accuracy: 0.9816\n",
        "Epoch 3/100\n",
        "877/877 [==============================] - 4s 4ms/step - loss: 0.0924 - accuracy: 0.9720 - val_loss: 0.0509 - val_accuracy: 0.9844\n",
        "Epoch 4/100\n",
        "877/877 [==============================] - 4s 4ms/step - loss: 0.0805 - accuracy: 0.9754 - val_loss: 0.0506 - val_accuracy: 0.9844\n",
        "Epoch 5/100\n",
        "877/877 [==============================] - 4s 5ms/step - loss: 0.0670 - accuracy: 0.9794 - val_loss: 0.0406 - val_accuracy: 0.9875\n",
        "Epoch 6/100\n",
        "877/877 [==============================] - 4s 5ms/step - loss: 0.0602 - accuracy: 0.9811 - val_loss: 0.0466 - val_accuracy: 0.9867\n",
        "Epoch 7/100\n",
        "877/877 [==============================] - 4s 5ms/step - loss: 0.0615 - accuracy: 0.9816 - val_loss: 0.0345 - val_accuracy: 0.9887\n",
        "Epoch 8/100\n",
        "877/877 [==============================] - 4s 4ms/step - loss: 0.0532 - accuracy: 0.9834 - val_loss: 0.0414 - val_accuracy: 0.9891\n",
        "Epoch 9/100\n",
        "877/877 [==============================] - 4s 4ms/step - loss: 0.0491 - accuracy: 0.9856 - val_loss: 0.0393 - val_accuracy: 0.9883\n",
        "Epoch 10/100\n",
        "877/877 [==============================] - 4s 4ms/step - loss: 0.0429 - accuracy: 0.9878 - val_loss: 0.0347 - val_accuracy: 0.9914\n",
        "Epoch 11/100\n",
        "877/877 [==============================] - 8s 9ms/step - loss: 0.0408 - accuracy: 0.9875 - val_loss: 0.0336 - val_accuracy: 0.9910\n",
        "Epoch 12/100\n",
        "877/877 [==============================] - 4s 4ms/step - loss: 0.0405 - accuracy: 0.9872 - val_loss: 0.0328 - val_accuracy: 0.9898\n",
        "Epoch 13/100\n",
        "877/877 [==============================] - 4s 4ms/step - loss: 0.0395 - accuracy: 0.9867 - val_loss: 0.0310 - val_accuracy: 0.9894\n",
        "Epoch 14/100\n",
        "877/877 [==============================] - 4s 4ms/step - loss: 0.0339 - accuracy: 0.9893 - val_loss: 0.0305 - val_accuracy: 0.9894\n",
        "Epoch 15/100\n",
        "877/877 [==============================] - 4s 4ms/step - loss: 0.0320 - accuracy: 0.9903 - val_loss: 0.0315 - val_accuracy: 0.9906\n",
        "Epoch 16/100\n",
        "877/877 [==============================] - 4s 4ms/step - loss: 0.0369 - accuracy: 0.9889 - val_loss: 0.0379 - val_accuracy: 0.9906\n",
        "Epoch 17/100\n",
        "877/877 [==============================] - 4s 4ms/step - loss: 0.0341 - accuracy: 0.9889 - val_loss: 0.0292 - val_accuracy: 0.9922\n",
        "Epoch 18/100\n",
        "877/877 [==============================] - 4s 4ms/step - loss: 0.0321 - accuracy: 0.9900 - val_loss: 0.0308 - val_accuracy: 0.9910\n",
        "Epoch 19/100\n",
        "877/877 [==============================] - 4s 4ms/step - loss: 0.0292 - accuracy: 0.9915 - val_loss: 0.0309 - val_accuracy: 0.9906\n",
        "Epoch 20/100\n",
        "877/877 [==============================] - 4s 4ms/step - loss: 0.0284 - accuracy: 0.9906 - val_loss: 0.0285 - val_accuracy: 0.9914\n",
        "Epoch 21/100\n",
        "877/877 [==============================] - 4s 4ms/step - loss: 0.0260 - accuracy: 0.9913 - val_loss: 0.0264 - val_accuracy: 0.9918\n",
        "Epoch 22/100\n",
        "877/877 [==============================] - 4s 4ms/step - loss: 0.0267 - accuracy: 0.9916 - val_loss: 0.0284 - val_accuracy: 0.9922\n",
        "Epoch 23/100\n",
        "877/877 [==============================] - 4s 4ms/step - loss: 0.0277 - accuracy: 0.9912 - val_loss: 0.0275 - val_accuracy: 0.9937\n",
        "Epoch 24/100\n",
        "877/877 [==============================] - 4s 4ms/step - loss: 0.0243 - accuracy: 0.9931 - val_loss: 0.0268 - val_accuracy: 0.9910\n",
        "Epoch 25/100\n",
        "877/877 [==============================] - 4s 4ms/step - loss: 0.0245 - accuracy: 0.9923 - val_loss: 0.0317 - val_accuracy: 0.9914\n",
        "Epoch 26/100\n",
        "877/877 [==============================] - 4s 4ms/step - loss: 0.0235 - accuracy: 0.9937 - val_loss: 0.0361 - val_accuracy: 0.9898\n",
        "Epoch 27/100\n",
        "877/877 [==============================] - 4s 4ms/step - loss: 0.0205 - accuracy: 0.9935 - val_loss: 0.0250 - val_accuracy: 0.9918\n",
        "Epoch 28/100\n",
        "877/877 [==============================] - 4s 4ms/step - loss: 0.0218 - accuracy: 0.9940 - val_loss: 0.0281 - val_accuracy: 0.9937\n",
        "Epoch 29/100\n",
        "877/877 [==============================] - 4s 4ms/step - loss: 0.0235 - accuracy: 0.9936 - val_loss: 0.0211 - val_accuracy: 0.9926\n",
        "Epoch 30/100\n",
        "877/877 [==============================] - 4s 4ms/step - loss: 0.0261 - accuracy: 0.9921 - val_loss: 0.0319 - val_accuracy: 0.9906\n",
        "Epoch 31/100\n",
        "877/877 [==============================] - 4s 4ms/step - loss: 0.0243 - accuracy: 0.9928 - val_loss: 0.0352 - val_accuracy: 0.9906\n",
        "Epoch 32/100\n",
        "877/877 [==============================] - 4s 4ms/step - loss: 0.0220 - accuracy: 0.9936 - val_loss: 0.0360 - val_accuracy: 0.9906\n",
        "Epoch 33/100\n",
        "877/877 [==============================] - 4s 4ms/step - loss: 0.0196 - accuracy: 0.9936 - val_loss: 0.0291 - val_accuracy: 0.9922\n",
        "Epoch 34/100\n",
        "877/877 [==============================] - 4s 4ms/step - loss: 0.0213 - accuracy: 0.9941 - val_loss: 0.0295 - val_accuracy: 0.9926\n",
        "Epoch 35/100\n",
        "877/877 [==============================] - 4s 4ms/step - loss: 0.0207 - accuracy: 0.9950 - val_loss: 0.0271 - val_accuracy: 0.9937\n",
        "Epoch 36/100\n",
        "877/877 [==============================] - 4s 5ms/step - loss: 0.0194 - accuracy: 0.9940 - val_loss: 0.0226 - val_accuracy: 0.9934\n",
        "Epoch 37/100\n",
        "877/877 [==============================] - 4s 4ms/step - loss: 0.0201 - accuracy: 0.9944 - val_loss: 0.0310 - val_accuracy: 0.9930\n",
        "Epoch 38/100\n",
        "877/877 [==============================] - 4s 4ms/step - loss: 0.0211 - accuracy: 0.9944 - val_loss: 0.0293 - val_accuracy: 0.9922\n",
        "Epoch 39/100\n",
        "877/877 [==============================] - 4s 4ms/step - loss: 0.0211 - accuracy: 0.9941 - val_loss: 0.0335 - val_accuracy: 0.9930\n",
        "plot_learning_curves(history_bn, label=\"Batch Normalization\")\n",
        "plot_learning_curves(history_dropout, label=\"Dropout\")\n",
        "\n",
        "\n",
        "2. Transfer learning.\n",
        "\n",
        "a. Create a new DNN that reuses all the pretrained hidden layers of the previous model, freezes them, and replaces the softmax output layer with a new one.\n",
        "\n",
        "b. Train this new DNN on digits 5 to 9, using only 100 images per digit, and time how long it takes. Despite this small number of examples, can you achieve high precision?\n",
        "\n",
        "c. Try caching the frozen layers, and train the model again: how much faster is it now?\n",
        "\n",
        "d. Try again reusing just four hidden layers instead of five. Can you achieve a higher precision?\n",
        "\n",
        "e. Now unfreeze the top two hidden layers and continue training: can you get the model to perform even better?\n",
        "\n",
        "Ans:\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "# Load the pretrained model\n",
        "pretrained_model = keras.models.load_model(\"my_mnist_model.h5\")\n",
        "pretrained_model.summary()\n",
        "Model: \"sequential_1\"\n",
        "_________________________________________________________________\n",
        "Layer (type)                 Output Shape              Param #   \n",
        "=================================================================\n",
        "flatten_1 (Flatten)          (None, 784)               0         \n",
        "_________________________________________________________________\n",
        "dense_6 (Dense)              (None, 300)               235500    \n",
        "_________________________________________________________________\n",
        "dense_7 (Dense)              (None, 100)               30100     \n",
        "_________________________________________________________________\n",
        "dense_8 (Dense)              (None, 5)                 505       \n",
        "=================================================================\n",
        "Total params: 266,105\n",
        "Trainable params: 266,105\n",
        "Non-trainable params: 0\n",
        "_________________________________________________________________\n",
        "# Create a new DNN that reuses all the pretrained hidden layers of the previous model\n",
        "new_model = keras.models.Sequential(pretrained_model.layers[:-1])\n",
        "# Freeze the reused layers\n",
        "for layer in new_model.layers:\n",
        "    layer.trainable = False\n",
        "# Add a new softmax output layer with the desired number of units\n",
        "new_model.add(keras.layers.Dense(5, activation=\"softmax\"))\n",
        "# Compile the new model\n",
        "new_model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"sgd\", metrics=[\"accuracy\"])\n",
        "# Load the training data for digits 5 to 9\n",
        "(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()\n",
        "X_train = X_train[y_train >= 5]\n",
        "y_train = y_train[y_train >= 5] - 5\n",
        "X_test = X_test[y_test >= 5]\n",
        "y_test = y_test[y_test >= 5] - 5\n",
        "# Train the new model on digits 5 to 9 using only 100 images per digit\n",
        "history = new_model.fit(X_train[:500], y_train[:500], epochs=10, validation_data=(X_test, y_test))\n",
        "Epoch 1/10\n",
        "16/16 [==============================] - 1s 42ms/step - loss: 6631.8728 - accuracy: 0.4171 - val_loss: 6194.2808 - val_accuracy: 0.5721\n",
        "Epoch 2/10\n",
        "16/16 [==============================] - 0s 25ms/step - loss: 2835.1217 - accuracy: 0.6859 - val_loss: 7178.7729 - val_accuracy: 0.5431\n",
        "Epoch 3/10\n",
        "16/16 [==============================] - 0s 27ms/step - loss: 2470.4009 - accuracy: 0.7279 - val_loss: 2398.1023 - val_accuracy: 0.6910\n",
        "Epoch 4/10\n",
        "16/16 [==============================] - 0s 26ms/step - loss: 1480.4643 - accuracy: 0.7750 - val_loss: 4813.9907 - val_accuracy: 0.4822\n",
        "Epoch 5/10\n",
        "16/16 [==============================] - 0s 26ms/step - loss: 1609.5031 - accuracy: 0.7378 - val_loss: 1787.2045 - val_accuracy: 0.7326\n",
        "Epoch 6/10\n",
        "16/16 [==============================] - 0s 26ms/step - loss: 1299.1982 - accuracy: 0.7901 - val_loss: 1083.7369 - val_accuracy: 0.8218\n",
        "Epoch 7/10\n",
        "16/16 [==============================] - 0s 26ms/step - loss: 956.8138 - accuracy: 0.8461 - val_loss: 2306.6565 - val_accuracy: 0.6840\n",
        "Epoch 8/10\n",
        "16/16 [==============================] - 0s 26ms/step - loss: 1378.5597 - accuracy: 0.7757 - val_loss: 890.4186 - val_accuracy: 0.8301\n",
        "Epoch 9/10\n",
        "16/16 [==============================] - 0s 26ms/step - loss: 738.2645 - accuracy: 0.8656 - val_loss: 1717.6547 - val_accuracy: 0.7702\n",
        "Epoch 10/10\n",
        "16/16 [==============================] - 0s 25ms/step - loss: 899.3345 - accuracy: 0.8144 - val_loss: 1324.8905 - val_accuracy: 0.7815\n",
        "# Evaluate the trained model\n",
        "test_loss, test_acc = new_model.evaluate(X_test, y_test)\n",
        "print(f\"Test accuracy: {test_acc}\")\n",
        "152/152 [==============================] - 0s 2ms/step - loss: 1324.8905 - accuracy: 0.7815\n",
        "Test accuracy: 0.7815264463424683\n",
        "# Try caching the frozen layers and train the model again\n",
        "new_model.layers[0].trainable = True\n",
        "new_model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"sgd\", metrics=[\"accuracy\"])\n",
        "history = new_model.fit(X_train[:500], y_train[:500], epochs=10, validation_data=(X_test, y_test))\n",
        "Epoch 1/10\n",
        "16/16 [==============================] - 1s 33ms/step - loss: 680.8155 - accuracy: 0.8695 - val_loss: 1106.4265 - val_accuracy: 0.8093\n",
        "Epoch 2/10\n",
        "16/16 [==============================] - 0s 28ms/step - loss: 278.7074 - accuracy: 0.8965 - val_loss: 1648.4424 - val_accuracy: 0.7429\n",
        "Epoch 3/10\n",
        "16/16 [==============================] - 0s 28ms/step - loss: 512.6315 - accuracy: 0.8894 - val_loss: 1288.6768 - val_accuracy: 0.7965\n",
        "Epoch 4/10\n",
        "16/16 [==============================] - 0s 29ms/step - loss: 908.9700 - accuracy: 0.8516 - val_loss: 1442.8612 - val_accuracy: 0.7519\n",
        "Epoch 5/10\n",
        "16/16 [==============================] - 1s 34ms/step - loss: 887.2769 - accuracy: 0.8483 - val_loss: 1312.6733 - val_accuracy: 0.8260\n",
        "Epoch 6/10\n",
        "16/16 [==============================] - 1s 34ms/step - loss: 519.5772 - accuracy: 0.8862 - val_loss: 3744.6335 - val_accuracy: 0.5674\n",
        "Epoch 7/10\n",
        "16/16 [==============================] - 1s 34ms/step - loss: 3351.1100 - accuracy: 0.6893 - val_loss: 1232.2354 - val_accuracy: 0.8393\n",
        "Epoch 8/10\n",
        "16/16 [==============================] - 1s 33ms/step - loss: 599.7999 - accuracy: 0.8975 - val_loss: 2660.2266 - val_accuracy: 0.6684\n",
        "Epoch 9/10\n",
        "16/16 [==============================] - 0s 33ms/step - loss: 1338.1793 - accuracy: 0.7826 - val_loss: 1329.4877 - val_accuracy: 0.8122\n",
        "Epoch 10/10\n",
        "16/16 [==============================] - 0s 32ms/step - loss: 437.1178 - accuracy: 0.9023 - val_loss: 1216.0264 - val_accuracy: 0.8128\n",
        "# Evaluate the trained model again\n",
        "test_loss, test_acc = new_model.evaluate(X_test, y_test)\n",
        "print(f\"Test accuracy: {test_acc}\")\n",
        "152/152 [==============================] - 0s 3ms/step - loss: 1216.0264 - accuracy: 0.8128\n",
        "Test accuracy: 0.8127956986427307\n",
        "# Try again reusing just four hidden layers instead of five\n",
        "new_model2 = keras.models.Sequential(pretrained_model.layers[:-2])\n",
        "for layer in new_model2.layers:\n",
        "    layer.trainable = False\n",
        "new_model2.add(keras.layers.Dense(5, activation=\"softmax\"))\n",
        "new_model2.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"sgd\", metrics=[\"accuracy\"])\n",
        "history = new_model2.fit(X_train[:500], y_train[:500], epochs=10, validation_data=(X_test, y_test))\n",
        "Epoch 1/10\n",
        "16/16 [==============================] - 1s 40ms/step - loss: 1811.4523 - accuracy: 0.5002 - val_loss: 318.5474 - val_accuracy: 0.8412\n",
        "Epoch 2/10\n",
        "16/16 [==============================] - 0s 32ms/step - loss: 386.5124 - accuracy: 0.8394 - val_loss: 247.1594 - val_accuracy: 0.8794\n",
        "Epoch 3/10\n",
        "16/16 [==============================] - 0s 33ms/step - loss: 126.0003 - accuracy: 0.9209 - val_loss: 247.7344 - val_accuracy: 0.8766\n",
        "Epoch 4/10\n",
        "16/16 [==============================] - 0s 33ms/step - loss: 48.0273 - accuracy: 0.9552 - val_loss: 635.1711 - val_accuracy: 0.7756\n",
        "Epoch 5/10\n",
        "16/16 [==============================] - 0s 32ms/step - loss: 92.9656 - accuracy: 0.9284 - val_loss: 343.5047 - val_accuracy: 0.8204\n",
        "Epoch 6/10\n",
        "16/16 [==============================] - 0s 33ms/step - loss: 47.8653 - accuracy: 0.9468 - val_loss: 204.4078 - val_accuracy: 0.8904\n",
        "Epoch 7/10\n",
        "16/16 [==============================] - 0s 32ms/step - loss: 12.7669 - accuracy: 0.9782 - val_loss: 211.0022 - val_accuracy: 0.8893\n",
        "Epoch 8/10\n",
        "16/16 [==============================] - 0s 32ms/step - loss: 18.6921 - accuracy: 0.9799 - val_loss: 192.8039 - val_accuracy: 0.8877\n",
        "Epoch 9/10\n",
        "16/16 [==============================] - 0s 32ms/step - loss: 8.2745 - accuracy: 0.9816 - val_loss: 213.9979 - val_accuracy: 0.8889\n",
        "Epoch 10/10\n",
        "16/16 [==============================] - 0s 32ms/step - loss: 10.1117 - accuracy: 0.9896 - val_loss: 252.5659 - val_accuracy: 0.8642\n",
        "# Evaluate the trained model again\n",
        "test_loss, test_acc = new_model2.evaluate(X_test, y_test)\n",
        "print(f\"Test accuracy: {test_acc}\")\n",
        "152/152 [==============================] - 0s 3ms/step - loss: 252.5659 - accuracy: 0.8642\n",
        "Test accuracy: 0.864225447177887\n",
        "# Now unfreeze the top two hidden layers and continue training\n",
        "for layer in new_model2.layers[-3:]:\n",
        "    layer.trainable = True\n",
        "new_model2.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"sgd\", metrics=[\"accuracy\"])\n",
        "history = new_model2.fit(X_train[:500], y_train[:500], epochs=10, validation_data=(X_test, y_test))\n",
        "Epoch 1/10\n",
        "16/16 [==============================] - 1s 50ms/step - loss: 708751.3557 - accuracy: 0.3699 - val_loss: 20.4435 - val_accuracy: 0.3730\n",
        "Epoch 2/10\n",
        "16/16 [==============================] - 1s 34ms/step - loss: 2.8107 - accuracy: 0.4070 - val_loss: 20.0421 - val_accuracy: 0.3793\n",
        "Epoch 3/10\n",
        "16/16 [==============================] - 1s 33ms/step - loss: 1.5865 - accuracy: 0.3780 - val_loss: 17.9182 - val_accuracy: 0.3841\n",
        "Epoch 4/10\n",
        "16/16 [==============================] - 0s 32ms/step - loss: 1.1696 - accuracy: 0.4147 - val_loss: 17.8571 - val_accuracy: 0.3709\n",
        "Epoch 5/10\n",
        "16/16 [==============================] - 0s 32ms/step - loss: 1.2136 - accuracy: 0.4326 - val_loss: 17.9410 - val_accuracy: 0.4040\n",
        "Epoch 6/10\n",
        "16/16 [==============================] - 0s 33ms/step - loss: 1.1793 - accuracy: 0.4350 - val_loss: 17.9059 - val_accuracy: 0.3853\n",
        "Epoch 7/10\n",
        "16/16 [==============================] - 0s 33ms/step - loss: 1.1676 - accuracy: 0.4477 - val_loss: 17.9039 - val_accuracy: 0.3863\n",
        "Epoch 8/10\n",
        "16/16 [==============================] - 0s 33ms/step - loss: 1.1587 - accuracy: 0.4586 - val_loss: 17.7386 - val_accuracy: 0.3874\n",
        "Epoch 9/10\n",
        "16/16 [==============================] - 0s 33ms/step - loss: 1.2716 - accuracy: 0.4122 - val_loss: 17.7381 - val_accuracy: 0.3882\n",
        "Epoch 10/10\n",
        "16/16 [==============================] - 0s 32ms/step - loss: 1.2341 - accuracy: 0.4335 - val_loss: 17.7346 - val_accuracy: 0.3880\n",
        "# Evaluate the trained model again\n",
        "test_loss, test_acc = new_model2.evaluate(X_test, y_test)\n",
        "print(f\"Test accuracy: {test_acc}\")\n",
        "152/152 [==============================] - 0s 3ms/step - loss: 17.7346 - accuracy: 0.3880\n",
        "Test accuracy: 0.3879860043525696\n",
        "3. Pretraining on an auxiliary task.\n",
        "\n",
        "a. In this exercise you will build a DNN that compares two MNIST digit images and predicts whether they represent the same digit or not. Then you will reuse the lower layers of this network to train an MNIST classifier using very little training data. Start by building two DNNs (lets call them DNN A and B), both similar to the one you built earlier but without the output layer: each DNN should have five hidden layers of 100 neurons each, He initialization, and ELU activation. Next, add one more hidden layer with 10 units on top of both DNNs. To do this, you should use TensorFlows concat() function with axis=1 to concatenate the outputs of both DNNs for each instance, then feed the result to the hidden layer. Finally, add an output layer with a single neuron using the logistic activation function.\n",
        "\n",
        "b. Split the MNIST training set in two sets: split #1 should containing 55,000 images, and split #2 should contain contain 5,000 images. Create a function that generates a training batch where each instance is a pair of MNIST images picked from split #1. Half of the training instances should be pairs of images that belong to the same class, while the other half should be images from different classes. For each pair, the training label should be 0 if the images are from the same class, or 1 if they are from different classes.\n",
        "\n",
        "c. Train the DNN on this training set. For each image pair, you can simultaneously feed the first image to DNN A and the second image to DNN B. The whole network will gradually learn to tell whether two images belong to the same class or not.\n",
        "\n",
        "d. Now create a new DNN by reusing and freezing the hidden layers of DNN A and adding a softmax output layer on top with 10 neurons. Train this network on split #2 and see if you can achieve high performance despite having only 500 images per class.\n",
        "\n",
        "Ans:\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "# Load the MNIST dataset and split it into two sets\n",
        "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.mnist.load_data()\n",
        "X_train_full = X_train_full.astype(np.float32) / 255\n",
        "X_test = X_test.astype(np.float32) / 255\n",
        "X_train1 = X_train_full[:55000]\n",
        "y_train1 = y_train_full[:55000]\n",
        "X_train2 = X_train_full[55000:]\n",
        "y_train2 = y_train_full[55000:]\n",
        "# Define a function to generate a training batch where each instance is a pair of MNIST images\n",
        "def generate_batch(batch_size):\n",
        "    X_batch1 = np.empty((batch_size, 28, 28), dtype=np.float32)\n",
        "    X_batch2 = np.empty((batch_size, 28, 28), dtype=np.float32)\n",
        "    y_batch = np.empty((batch_size, 1), dtype=np.int32)\n",
        "    for i in range(batch_size):\n",
        "        index1 = np.random.randint(len(X_train1))\n",
        "        X_batch1[i] = X_train1[index1]\n",
        "        if i % 2 == 0:\n",
        "            # Images from the same class\n",
        "            index2 = index1\n",
        "            while index2 == index1:\n",
        "                index2 = np.random.randint(len(X_train1))\n",
        "                if y_train1[index2] == y_train1[index1]:\n",
        "                    break\n",
        "            y_batch[i] = 0\n",
        "        else:\n",
        "            # Images from different classes\n",
        "            index2 = index1\n",
        "            while index2 == index1 or y_train1[index2] == y_train1[index1]:\n",
        "                index2 = np.random.randint(len(X_train1))\n",
        "            y_batch[i] = 1\n",
        "        X_batch2[i] = X_train1[index2]\n",
        "    return [X_batch1, X_batch2], y_batch\n",
        "# Build two DNNs (lets call them DNN A and B), both similar to the one you built earlier but without the output layer\n",
        "def build_dnn():\n",
        "    model = keras.models.Sequential()\n",
        "    model.add(keras.layers.Flatten(input_shape=[28, 28]))\n",
        "    for _ in range(5):\n",
        "        model.add(keras.layers.Dense(100, activation=\"elu\", kernel_initializer=\"he_normal\"))\n",
        "    return model\n",
        "\n",
        "dnn_a = build_dnn()\n",
        "dnn_b = build_dnn()\n",
        "# Add one more hidden layer with 10 units on top of both DNNs\n",
        "merged_output = keras.layers.concatenate([dnn_a.output, dnn_b.output])\n",
        "hidden_layer = keras.layers.Dense(10, activation=\"elu\", kernel_initializer=\"he_normal\")(merged_output)\n",
        "# Add an output layer with a single neuron using the logistic activation function\n",
        "output_layer = keras.layers.Dense(1, activation=\"sigmoid\")(hidden_layer)\n",
        "# Create a new model that takes two MNIST images as inputs and outputs a single value representing whether the images belong to the same class or not\n",
        "model = keras.models.Model(inputs=[dnn_a.input, dnn_b.input], outputs=[output_layer])\n",
        "# Compile the model\n",
        "model.compile(loss=\"binary_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
        "# Train the DNN on this training set\n",
        "batch_size = 32\n",
        "for epoch in range(10):\n",
        "    for iteration in range(len(X_train1) // batch_size):\n",
        "        X_batch, y_batch = generate_batch(batch_size)\n",
        "        model.train_on_batch(X_batch, y_batch)\n",
        "# Create a new DNN by reusing and freezing the hidden layers of DNN A and adding a softmax output layer on top with 10 neurons\n",
        "new_model = keras.models.Sequential(dnn_a.layers[:-1])\n",
        "for layer in new_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "new_model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
        "# Compile the new model\n",
        "new_model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
        "# Train this network on split #2 and see if you can achieve high performance despite having only 500 images per class\n",
        "history = new_model.fit(X_train2, y_train2, epochs=10)\n",
        "Epoch 1/10\n",
        "157/157 [==============================] - 1s 4ms/step - loss: 3.0181 - accuracy: 0.1600\n",
        "Epoch 2/10\n",
        "157/157 [==============================] - 1s 4ms/step - loss: 1.7712 - accuracy: 0.4140\n",
        "Epoch 3/10\n",
        "157/157 [==============================] - 1s 4ms/step - loss: 1.5623 - accuracy: 0.5175\n",
        "Epoch 4/10\n",
        "157/157 [==============================] - 1s 4ms/step - loss: 1.4562 - accuracy: 0.5426\n",
        "Epoch 5/10\n",
        "157/157 [==============================] - 1s 4ms/step - loss: 1.3350 - accuracy: 0.5818\n",
        "Epoch 6/10\n",
        "157/157 [==============================] - 1s 4ms/step - loss: 1.2855 - accuracy: 0.6100\n",
        "Epoch 7/10\n",
        "157/157 [==============================] - 1s 4ms/step - loss: 1.2436 - accuracy: 0.6162\n",
        "Epoch 8/10\n",
        "157/157 [==============================] - 1s 4ms/step - loss: 1.1790 - accuracy: 0.6500\n",
        "Epoch 9/10\n",
        "157/157 [==============================] - 1s 4ms/step - loss: 1.1295 - accuracy: 0.6686\n",
        "Epoch 10/10\n",
        "157/157 [==============================] - 1s 4ms/step - loss: 1.1135 - accuracy: 0.6761\n",
        "# Evaluate the trained model on the test set\n",
        "test_loss, test_acc = new_model.evaluate(X_test, y_test)\n",
        "print(f\"Test accuracy: {test_acc}\")\n",
        "313/313 [==============================] - 1s 2ms/step - loss: 1.1941 - accuracy: 0.6229\n",
        "Test accuracy: 0.6229000091552734"
      ],
      "metadata": {
        "id": "kOFQ4imrnLA6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "noluRGmQnCBQ"
      },
      "outputs": [],
      "source": []
    }
  ]
}