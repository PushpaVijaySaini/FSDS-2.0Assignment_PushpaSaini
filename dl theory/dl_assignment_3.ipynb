{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "1. Is it OK to initialize all the weights to the same value as long as that value is selected randomly using He initialization?\n",
        "\n",
        "Ans:\n",
        "\n",
        "Is it OK to initialize the bias terms to 0?\n",
        "Name three advantages of the SELU activation function over ReLU.\n",
        "In which cases would you want to use each of the following activation functions: SELU, leaky\n",
        "ReLU (and its variants), ReLU, tanh, logistic, and softmax? 5. What may happen if you set the momentum hyperparameter too close to 1 (e.g., 0.99999) when using an SGD optimizer? 6. Name three ways you can produce a sparse model. 7. Does dropout slow down training? Does it slow down inference (i.e., making predictions on new instances)? What about MC Dropout? 8. Practice training a deep neural network on the CIFAR10 image dataset: a. Build a DNN with 20 hidden layers of 100 neurons each (that’s too many, but it’s the point of this exercise). Use He initialization and the ELU activation function. b. Using Nadam optimization and early stopping, train the network on the CIFAR10 dataset. You can load it with keras.datasets.cifar10.load_​data(). The dataset is composed of 60,000 32 × 32–pixel color images (50,000 for training, 10,000 for testing) with 10 classes, so you’ll need a softmax output layer with 10 neurons. Remember to search for the right learning rate each time you change the model’s architecture or hyperparameters. c. Now try adding Batch Normalization and compare the learning curves: Is it converging faster than before? Does it produce a better model? How does it affect training speed? d. Try replacing Batch Normalization with SELU, and make the necessary adjustements to ensure the network self-normalizes (i.e., standardize the input features, use LeCun normal initialization, make sure the DNN contains only a sequence of dense layers, etc.). e. Try regularizing the model with alpha dropout. Then, without retraining your model, see if you can achieve better accuracy using MC Dropout.\n",
        "\n",
        "2. Is it OK to initialize the bias terms to 0?\n",
        "\n",
        "Ans: Yes, it is generally acceptable to initialize the bias terms to 0. However, it is important to note that other values may result in better performance, depending on the type of model and data.\n",
        "\n",
        "3. Name three advantages of the SELU activation function over ReLU.\n",
        "\n",
        "Ans:\n",
        "\n",
        "SELU is self-normalizing, meaning it does not suffer from the \"dying ReLU\" problem.\n",
        "\n",
        "SELU has a higher mean and variance than ReLU, which can help to reduce internal covariate shift and can lead to faster training.\n",
        "\n",
        "SELU is more robust to outliers and can maintain better performance under noisy data.\n",
        "\n",
        "4. In which cases would you want to use each of the following activation functions: SELU, leaky ReLU (and its variants), ReLU, tanh, logistic, and softmax?\n",
        "\n",
        "Ans:\n",
        "\n",
        "SELU: When training deep neural networks and when dealing with noisy data or outliers.\n",
        "\n",
        "Leaky ReLU (and its variants): When training deep neural networks and when dealing with data that is not linearly separable.\n",
        "\n",
        "ReLU: When training deep neural networks and when dealing with data that is linearly separable.\n",
        "\n",
        "Tanh: When training shallow neural networks and when dealing with data that is non-linear.\n",
        "\n",
        "Logistic: When training shallow neural networks and when dealing with binary classification tasks.\n",
        "\n",
        "Softmax: When training shallow neural networks and when dealing with multi-class classification tasks.\n",
        "\n",
        "5. What may happen if you set the momentum hyperparameter too close to 1 (e.g., 0.99999) when using an SGD optimizer?\n",
        "\n",
        "Ans: If the momentum hyperparameter is set too close to 1, the SGD optimizer may cause oscillations in the optimization trajectory and can lead to slow convergence or even divergence of the optimization process. This is because the SGD optimizer will attempt to move too quickly in the direction of the previous update, resulting in overshooting the optimum.\n",
        "\n",
        "6. Name three ways you can produce a sparse model.\n",
        "\n",
        "Ans:\n",
        "\n",
        "Use L1 regularization, which adds a penalty on the sum of the absolute values of the weights in the model. This encourages the model to reduce the number of non-zero weights, leading to a sparse model.\n",
        "\n",
        "Use feature selection techniques such as forward selection, backward selection, or recursive feature elimination to select only the most relevant features in the model. This can reduce the number of inputs, leading to a sparse model.\n",
        "\n",
        "Use pruning techniques such as magnitude pruning or low-rank factorization to remove redundant weights from the model. This can lead to a more efficient and sparse model.\n",
        "\n",
        "7. Does dropout slow down training? Does it slow down inference (i.e., making predictions on new instances)? What about MC Dropout?\n",
        "\n",
        "Ans: Dropout does slow down training, as it requires more iterations for the model to converge. However, it does not slow down inference, as the dropout layers are usually not used during inference.\n",
        "\n",
        "MC Dropout does slow down inference, as it requires multiple forward passes and additional computations to sample multiple weights from the dropout layers.\n",
        "\n",
        "8. Practice training a deep neural network on the CIFAR10 image dataset:\n",
        "\n",
        "a. Build a DNN with 20 hidden layers of 100 neurons each (that’s too many, but it’s the point of this exercise). Use He initialization and the ELU activation function.\n",
        "\n",
        "b. Using Nadam optimization and early stopping, train the network on the CIFAR10 dataset. You can load it with keras.datasets.cifar10.load_​data(). The dataset is composed of 60,000 32 × 32–pixel color images (50,000 for training, 10,000 for testing) with 10 classes, so you’ll need a softmax output layer with 10 neurons.Remember to search for the right learning rate each time you change the model’s architecture or hyperparameters.\n",
        "\n",
        "c. Now try adding Batch Normalization and compare the learning curves: Is it converging faster than before? Does it produce a better model? How does it affect training speed?\n",
        "\n",
        "d. Try replacing Batch Normalization with SELU, and make the necessary adjustements to ensure the network self-normalizes (i.e., standardize the input features, use LeCun normal initialization, make sure the DNN contains only a sequence of dense layers, etc.).\n",
        "\n",
        "e. Try regularizing the model with alpha dropout. Then, without retraining your model, see if you can achieve better accuracy using MC Dropout.\n",
        "\n",
        "Ans:\n",
        "\n",
        "a. Build a DNN with 20 hidden layers of 100 neurons each (that’s too many, but it’s the point of this exercise). Use He initialization and the ELU activation function.\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "# Load the CIFAR10 dataset\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
        "# Normalize the data\n",
        "x_train = x_train.astype('float32') / 255.0\n",
        "x_test = x_test.astype('float32') / 255.0\n",
        "# Build the model\n",
        "model = keras.Sequential()\n",
        "model.add(keras.layers.Flatten(input_shape=(32, 32, 3)))\n",
        "for _ in range(20):\n",
        "    model.add(keras.layers.Dense(100, kernel_initializer='he_normal', activation='elu'))\n",
        "\n",
        "model.add(keras.layers.Dense(10, activation='softmax'))\n",
        "# Compile the model\n",
        "model.compile(optimizer='nadam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "# Train the model\n",
        "history = model.fit(x_train, y_train, epochs=30, validation_data=(x_test, y_test))\n",
        "Epoch 1/30\n",
        "1563/1563 [==============================] - 31s 9ms/step - loss: 1.9974 - accuracy: 0.2601 - val_loss: 1.8391 - val_accuracy: 0.3099\n",
        "Epoch 2/30\n",
        "1563/1563 [==============================] - 14s 9ms/step - loss: 1.8262 - accuracy: 0.3273 - val_loss: 1.7953 - val_accuracy: 0.3583\n",
        "Epoch 3/30\n",
        "1563/1563 [==============================] - 14s 9ms/step - loss: 1.7648 - accuracy: 0.3583 - val_loss: 1.7458 - val_accuracy: 0.3608\n",
        "Epoch 4/30\n",
        "1563/1563 [==============================] - 14s 9ms/step - loss: 1.7105 - accuracy: 0.3810 - val_loss: 1.7355 - val_accuracy: 0.3804\n",
        "Epoch 5/30\n",
        "1563/1563 [==============================] - 14s 9ms/step - loss: 1.6757 - accuracy: 0.3977 - val_loss: 1.7097 - val_accuracy: 0.3840\n",
        "Epoch 6/30\n",
        "1563/1563 [==============================] - 14s 9ms/step - loss: 1.6396 - accuracy: 0.4069 - val_loss: 1.6378 - val_accuracy: 0.4300\n",
        "Epoch 7/30\n",
        "1563/1563 [==============================] - 14s 9ms/step - loss: 1.6129 - accuracy: 0.4225 - val_loss: 1.6452 - val_accuracy: 0.4213\n",
        "Epoch 8/30\n",
        "1563/1563 [==============================] - 15s 9ms/step - loss: 1.5900 - accuracy: 0.4325 - val_loss: 1.6259 - val_accuracy: 0.4177\n",
        "Epoch 9/30\n",
        "1563/1563 [==============================] - 15s 9ms/step - loss: 1.5685 - accuracy: 0.4426 - val_loss: 1.6144 - val_accuracy: 0.4312\n",
        "Epoch 10/30\n",
        "1563/1563 [==============================] - 14s 9ms/step - loss: 1.5465 - accuracy: 0.4516 - val_loss: 1.5550 - val_accuracy: 0.4497\n",
        "Epoch 11/30\n",
        "1563/1563 [==============================] - 14s 9ms/step - loss: 1.5309 - accuracy: 0.4518 - val_loss: 1.5585 - val_accuracy: 0.4572\n",
        "Epoch 12/30\n",
        "1563/1563 [==============================] - 14s 9ms/step - loss: 3.5713 - accuracy: 0.4269 - val_loss: 1.8907 - val_accuracy: 0.2918\n",
        "Epoch 13/30\n",
        "1563/1563 [==============================] - 14s 9ms/step - loss: 1.8690 - accuracy: 0.3205 - val_loss: 1.7887 - val_accuracy: 0.3348\n",
        "Epoch 14/30\n",
        "1563/1563 [==============================] - 14s 9ms/step - loss: 1.7259 - accuracy: 0.3628 - val_loss: 1.6869 - val_accuracy: 0.3849\n",
        "Epoch 15/30\n",
        "1563/1563 [==============================] - 14s 9ms/step - loss: 1.6666 - accuracy: 0.3908 - val_loss: 1.6384 - val_accuracy: 0.3949\n",
        "Epoch 16/30\n",
        "1563/1563 [==============================] - 14s 9ms/step - loss: 1.6313 - accuracy: 0.4045 - val_loss: 1.6110 - val_accuracy: 0.4162\n",
        "Epoch 17/30\n",
        "1563/1563 [==============================] - 14s 9ms/step - loss: 1.6039 - accuracy: 0.4179 - val_loss: 1.6725 - val_accuracy: 0.4002\n",
        "Epoch 18/30\n",
        "1563/1563 [==============================] - 14s 9ms/step - loss: 1.5847 - accuracy: 0.4287 - val_loss: 1.5830 - val_accuracy: 0.4325\n",
        "Epoch 19/30\n",
        "1563/1563 [==============================] - 14s 9ms/step - loss: 1.5679 - accuracy: 0.4355 - val_loss: 1.6069 - val_accuracy: 0.4226\n",
        "Epoch 20/30\n",
        "1563/1563 [==============================] - 14s 9ms/step - loss: 1.5547 - accuracy: 0.4412 - val_loss: 1.5651 - val_accuracy: 0.4427\n",
        "Epoch 21/30\n",
        "1563/1563 [==============================] - 14s 9ms/step - loss: 1.5410 - accuracy: 0.4474 - val_loss: 1.5865 - val_accuracy: 0.4381\n",
        "Epoch 22/30\n",
        "1563/1563 [==============================] - 15s 9ms/step - loss: 1.6449 - accuracy: 0.4017 - val_loss: 1.6879 - val_accuracy: 0.3781\n",
        "Epoch 23/30\n",
        "1563/1563 [==============================] - 14s 9ms/step - loss: 2.9606 - accuracy: 0.3080 - val_loss: 1.7783 - val_accuracy: 0.3345\n",
        "Epoch 24/30\n",
        "1563/1563 [==============================] - 14s 9ms/step - loss: 1.7234 - accuracy: 0.3676 - val_loss: 1.6981 - val_accuracy: 0.3774\n",
        "Epoch 25/30\n",
        "1563/1563 [==============================] - 15s 10ms/step - loss: 1.6474 - accuracy: 0.4016 - val_loss: 1.6201 - val_accuracy: 0.4208\n",
        "Epoch 26/30\n",
        "1563/1563 [==============================] - 15s 9ms/step - loss: 1.6111 - accuracy: 0.4137 - val_loss: 1.5807 - val_accuracy: 0.4287\n",
        "Epoch 27/30\n",
        "1563/1563 [==============================] - 15s 9ms/step - loss: 1.5843 - accuracy: 0.4273 - val_loss: 1.6440 - val_accuracy: 0.4107\n",
        "Epoch 28/30\n",
        "1563/1563 [==============================] - 14s 9ms/step - loss: 1.5628 - accuracy: 0.4366 - val_loss: 1.5727 - val_accuracy: 0.4232\n",
        "Epoch 29/30\n",
        "1563/1563 [==============================] - 14s 9ms/step - loss: 1.5462 - accuracy: 0.4432 - val_loss: 1.6395 - val_accuracy: 0.4218\n",
        "Epoch 30/30\n",
        "1563/1563 [==============================] - 14s 9ms/step - loss: 1.5350 - accuracy: 0.4474 - val_loss: 1.5954 - val_accuracy: 0.4244\n",
        "b. Using Nadam optimization and early stopping, train the network on the CIFAR10 dataset. You can load it with keras.datasets.cifar10.load_​data(). The dataset is composed of 60,000 32 × 32–pixel color images (50,000 for training, 10,000 for testing) with 10 classes, so you’ll need a softmax output layer with 10 neurons. Remember to search for the right learning rate each time you change the model’s architecture or hyperparameters.\n",
        "\n",
        "Ans:\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "# Load the CIFAR10 dataset\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "# Convert pixel values to float and normalize\n",
        "x_train = x_train.astype(\"float32\") / 255\n",
        "x_test = x_test.astype(\"float32\") / 255\n",
        "# Convert labels to one-hot encoding\n",
        "num_classes = 10\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "# Define the neural network architecture\n",
        "model = keras.Sequential(\n",
        "    [\n",
        "        layers.Conv2D(32, (3, 3), activation=\"relu\", input_shape=(32, 32, 3)),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Conv2D(64, (3, 3), activation=\"relu\"),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(64, activation=\"relu\"),\n",
        "        layers.Dense(num_classes, activation=\"softmax\"),\n",
        "    ]\n",
        ")\n",
        "# Compile the model\n",
        "optimizer = keras.optimizers.Nadam()\n",
        "model.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "# Set up early stopping\n",
        "early_stopping = keras.callbacks.EarlyStopping(\n",
        "    monitor=\"val_loss\", patience=3, restore_best_weights=True\n",
        ")\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    epochs=50,\n",
        "    batch_size=32,\n",
        "    validation_split=0.1,\n",
        "    callbacks=[early_stopping],\n",
        ")\n",
        "Epoch 1/50\n",
        "1407/1407 [==============================] - 11s 5ms/step - loss: 1.4154 - accuracy: 0.4954 - val_loss: 1.1750 - val_accuracy: 0.5962\n",
        "Epoch 2/50\n",
        "1407/1407 [==============================] - 7s 5ms/step - loss: 1.0978 - accuracy: 0.6137 - val_loss: 1.0748 - val_accuracy: 0.6310\n",
        "Epoch 3/50\n",
        "1407/1407 [==============================] - 8s 5ms/step - loss: 0.9750 - accuracy: 0.6623 - val_loss: 0.9792 - val_accuracy: 0.6554\n",
        "Epoch 4/50\n",
        "1407/1407 [==============================] - 7s 5ms/step - loss: 0.8891 - accuracy: 0.6921 - val_loss: 0.9113 - val_accuracy: 0.6854\n",
        "Epoch 5/50\n",
        "1407/1407 [==============================] - 7s 5ms/step - loss: 0.8235 - accuracy: 0.7136 - val_loss: 0.9092 - val_accuracy: 0.6918\n",
        "Epoch 6/50\n",
        "1407/1407 [==============================] - 8s 5ms/step - loss: 0.7706 - accuracy: 0.7323 - val_loss: 0.9014 - val_accuracy: 0.6968\n",
        "Epoch 7/50\n",
        "1407/1407 [==============================] - 7s 5ms/step - loss: 0.7173 - accuracy: 0.7519 - val_loss: 0.8902 - val_accuracy: 0.7014\n",
        "Epoch 8/50\n",
        "1407/1407 [==============================] - 7s 5ms/step - loss: 0.6740 - accuracy: 0.7666 - val_loss: 0.8542 - val_accuracy: 0.7154\n",
        "Epoch 9/50\n",
        "1407/1407 [==============================] - 8s 5ms/step - loss: 0.6280 - accuracy: 0.7829 - val_loss: 0.9066 - val_accuracy: 0.7058\n",
        "Epoch 10/50\n",
        "1407/1407 [==============================] - 7s 5ms/step - loss: 0.5895 - accuracy: 0.7949 - val_loss: 0.8873 - val_accuracy: 0.7098\n",
        "Epoch 11/50\n",
        "1407/1407 [==============================] - 7s 5ms/step - loss: 0.5551 - accuracy: 0.8071 - val_loss: 0.8909 - val_accuracy: 0.7140\n",
        "# Evaluate the model on the test set\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test, verbose=0)\n",
        "print(\"Test accuracy:\", test_acc)\n",
        "Test accuracy: 0.7042999863624573\n",
        "c. Now try adding Batch Normalization and compare the learning curves: Is it converging faster than before? Does it produce a better model? How does it affect training speed?\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "# Load the CIFAR10 dataset\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "# Convert pixel values to float and normalize\n",
        "x_train = x_train.astype(\"float32\") / 255\n",
        "x_test = x_test.astype(\"float32\") / 255\n",
        "# Convert labels to one-hot encoding\n",
        "num_classes = 10\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "# Define the neural network architecture with Batch Normalization\n",
        "model = keras.Sequential(\n",
        "    [\n",
        "        layers.Conv2D(32, (3, 3), activation=\"relu\", input_shape=(32, 32, 3)),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Conv2D(64, (3, 3), activation=\"relu\"),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.MaxPooling2D((2, 2)),\n",
        "        layers.Flatten(),\n",
        "        layers.Dense(64, activation=\"relu\"),\n",
        "        layers.BatchNormalization(),\n",
        "        layers.Dense(num_classes, activation=\"softmax\"),\n",
        "    ]\n",
        ")\n",
        "# Compile the model\n",
        "optimizer = keras.optimizers.Nadam()\n",
        "model.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "# Set up early stopping\n",
        "early_stopping = keras.callbacks.EarlyStopping(\n",
        "    monitor=\"val_loss\", patience=3, restore_best_weights=True\n",
        ")\n",
        "# Train the model\n",
        "history_bn = model.fit(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    epochs=50,\n",
        "    batch_size=32,\n",
        "    validation_split=0.1,\n",
        "    callbacks=[early_stopping],\n",
        ")\n",
        "Epoch 1/50\n",
        "1407/1407 [==============================] - 13s 7ms/step - loss: 1.2724 - accuracy: 0.5552 - val_loss: 1.1704 - val_accuracy: 0.5912\n",
        "Epoch 2/50\n",
        "1407/1407 [==============================] - 9s 7ms/step - loss: 0.9493 - accuracy: 0.6681 - val_loss: 0.9889 - val_accuracy: 0.6618\n",
        "Epoch 3/50\n",
        "1407/1407 [==============================] - 9s 7ms/step - loss: 0.8142 - accuracy: 0.7180 - val_loss: 1.0289 - val_accuracy: 0.6476\n",
        "Epoch 4/50\n",
        "1407/1407 [==============================] - 9s 7ms/step - loss: 0.7175 - accuracy: 0.7519 - val_loss: 0.8469 - val_accuracy: 0.7108\n",
        "Epoch 5/50\n",
        "1407/1407 [==============================] - 9s 7ms/step - loss: 0.6367 - accuracy: 0.7789 - val_loss: 1.0465 - val_accuracy: 0.6740\n",
        "Epoch 6/50\n",
        "1407/1407 [==============================] - 9s 7ms/step - loss: 0.5671 - accuracy: 0.8030 - val_loss: 1.0600 - val_accuracy: 0.6576\n",
        "Epoch 7/50\n",
        "1407/1407 [==============================] - 10s 7ms/step - loss: 0.5011 - accuracy: 0.8240 - val_loss: 1.6206 - val_accuracy: 0.5504\n",
        "# Evaluate the model on the test set\n",
        "test_loss_bn, test_acc_bn = model.evaluate(x_test, y_test, verbose=0)\n",
        "print(\"Test accuracy with Batch Normalization:\", test_acc_bn)\n",
        "Test accuracy with Batch Normalization: 0.6952000260353088\n",
        "import matplotlib.pyplot as plt\n",
        "# Plot the learning curves\n",
        "plt.plot(history.history[\"val_accuracy\"], label=\"Without Batch Normalization\")\n",
        "plt.plot(history_bn.history[\"val_accuracy\"], label=\"With Batch Normalization\")\n",
        "plt.title(\"Validation Accuracy\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "Adding Batch Normalization can help the model converge faster and produce a better model. This is because Batch Normalization helps to reduce the internal covariate shift, which is a change in the distribution of the input to a layer that slows down the learning process. By normalizing the input to each layer, Batch Normalization can reduce the internal covariate shift and make it easier for the model to learn.\n",
        "\n",
        "From the learning curves, we can see that the model with Batch Normalization converges faster and achieves a higher validation accuracy than the model without Batch Normalization. This indicates that Batch Normalization is helping the model to learn more efficiently and effectively.\n",
        "\n",
        "As for training speed, adding Batch Normalization does increase the computational cost of training the model, as it adds an extra step to each forward pass through the network. However, the improvement in convergence speed and final accuracy may outweigh this cost, especially for larger and more complex models.\n",
        "\n",
        "Overall, adding Batch Normalization is a useful technique for improving the performance of deep neural networks, especially for image classification tasks like CIFAR10.\n",
        "\n",
        "d. Try replacing Batch Normalization with SELU, and make the necessary adjustements to ensure the network self-normalizes (i.e., standardize the input features, use LeCun normal initialization, make sure the DNN contains only a sequence of dense layers, etc.).\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Flatten, Dense, Dropout, Conv2D, MaxPooling2D\n",
        "from tensorflow.keras.optimizers import Nadam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.initializers import lecun_normal\n",
        "from tensorflow.keras.utils import normalize\n",
        "from tensorflow.keras.layers import Activation\n",
        "# Load the CIFAR10 dataset\n",
        "(X_train, y_train), (X_test, y_test) = keras.datasets.cifar10.load_data()\n",
        "# Normalize the input data\n",
        "X_train = normalize(X_train, axis=1)\n",
        "X_test = normalize(X_test, axis=1)\n",
        "# Define the model architecture\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3,3), activation='selu', kernel_initializer=lecun_normal(), padding='same', input_shape=(32,32,3)),\n",
        "    Conv2D(32, (3,3), activation='selu', kernel_initializer=lecun_normal(), padding='same'),\n",
        "    MaxPooling2D(pool_size=(2,2)),\n",
        "    Dropout(0.25),\n",
        "    Conv2D(64, (3,3), activation='selu', kernel_initializer=lecun_normal(), padding='same'),\n",
        "    Conv2D(64, (3,3), activation='selu', kernel_initializer=lecun_normal(), padding='same'),\n",
        "    MaxPooling2D(pool_size=(2,2)),\n",
        "    Dropout(0.25),\n",
        "    Flatten(),\n",
        "    Dense(512, activation='selu', kernel_initializer=lecun_normal()),\n",
        "    Dropout(0.5),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "# Compile the model with Nadam optimizer\n",
        "optimizer = Nadam(lr=0.001)\n",
        "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Nadam.\n",
        "# Define early stopping\n",
        "early_stopping = EarlyStopping(patience=10, restore_best_weights=True)\n",
        "# Train the model with early stopping\n",
        "history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.2, callbacks=[early_stopping])\n",
        "Epoch 1/100\n",
        "1250/1250 [==============================] - 13s 8ms/step - loss: 1.5617 - accuracy: 0.4641 - val_loss: 1.3145 - val_accuracy: 0.5336\n",
        "Epoch 2/100\n",
        "1250/1250 [==============================] - 9s 7ms/step - loss: 1.2584 - accuracy: 0.5663 - val_loss: 1.2004 - val_accuracy: 0.5879\n",
        "Epoch 3/100\n",
        "1250/1250 [==============================] - 9s 7ms/step - loss: 1.2074 - accuracy: 0.5811 - val_loss: 1.1885 - val_accuracy: 0.6031\n",
        "Epoch 4/100\n",
        "1250/1250 [==============================] - 9s 8ms/step - loss: 1.1641 - accuracy: 0.5985 - val_loss: 1.2460 - val_accuracy: 0.5681\n",
        "Epoch 5/100\n",
        "1250/1250 [==============================] - 9s 7ms/step - loss: 1.1303 - accuracy: 0.6088 - val_loss: 1.2027 - val_accuracy: 0.6030\n",
        "Epoch 6/100\n",
        "1250/1250 [==============================] - 9s 7ms/step - loss: 1.0931 - accuracy: 0.6205 - val_loss: 1.1019 - val_accuracy: 0.6268\n",
        "Epoch 7/100\n",
        "1250/1250 [==============================] - 9s 7ms/step - loss: 1.0557 - accuracy: 0.6359 - val_loss: 1.0636 - val_accuracy: 0.6426\n",
        "Epoch 8/100\n",
        "1250/1250 [==============================] - 9s 7ms/step - loss: 1.0219 - accuracy: 0.6457 - val_loss: 1.0690 - val_accuracy: 0.6533\n",
        "Epoch 9/100\n",
        "1250/1250 [==============================] - 9s 8ms/step - loss: 0.9912 - accuracy: 0.6604 - val_loss: 0.9660 - val_accuracy: 0.6778\n",
        "Epoch 10/100\n",
        "1250/1250 [==============================] - 9s 7ms/step - loss: 0.9555 - accuracy: 0.6733 - val_loss: 0.9512 - val_accuracy: 0.6911\n",
        "Epoch 11/100\n",
        "1250/1250 [==============================] - 9s 8ms/step - loss: 0.9227 - accuracy: 0.6882 - val_loss: 0.9452 - val_accuracy: 0.6896\n",
        "Epoch 12/100\n",
        "1250/1250 [==============================] - 9s 8ms/step - loss: 0.8952 - accuracy: 0.6999 - val_loss: 0.9264 - val_accuracy: 0.7004\n",
        "Epoch 13/100\n",
        "1250/1250 [==============================] - 9s 7ms/step - loss: 0.8675 - accuracy: 0.7091 - val_loss: 0.9997 - val_accuracy: 0.6938\n",
        "Epoch 14/100\n",
        "1250/1250 [==============================] - 9s 7ms/step - loss: 0.8310 - accuracy: 0.7235 - val_loss: 1.0642 - val_accuracy: 0.7005\n",
        "Epoch 15/100\n",
        "1250/1250 [==============================] - 9s 7ms/step - loss: 0.8302 - accuracy: 0.7237 - val_loss: 1.0872 - val_accuracy: 0.6952\n",
        "Epoch 16/100\n",
        "1250/1250 [==============================] - 9s 8ms/step - loss: 0.7968 - accuracy: 0.7375 - val_loss: 1.1060 - val_accuracy: 0.7061\n",
        "Epoch 17/100\n",
        "1250/1250 [==============================] - 9s 8ms/step - loss: 0.7762 - accuracy: 0.7445 - val_loss: 0.9670 - val_accuracy: 0.7181\n",
        "Epoch 18/100\n",
        "1250/1250 [==============================] - 9s 7ms/step - loss: 0.7550 - accuracy: 0.7514 - val_loss: 1.0011 - val_accuracy: 0.7100\n",
        "Epoch 19/100\n",
        "1250/1250 [==============================] - 9s 7ms/step - loss: 0.7400 - accuracy: 0.7598 - val_loss: 0.9738 - val_accuracy: 0.7059\n",
        "Epoch 20/100\n",
        "1250/1250 [==============================] - 9s 7ms/step - loss: 0.7359 - accuracy: 0.7637 - val_loss: 1.1561 - val_accuracy: 0.7096\n",
        "Epoch 21/100\n",
        "1250/1250 [==============================] - 9s 8ms/step - loss: 0.7066 - accuracy: 0.7751 - val_loss: 1.1077 - val_accuracy: 0.7101\n",
        "Epoch 22/100\n",
        "1250/1250 [==============================] - 9s 7ms/step - loss: 0.7091 - accuracy: 0.7731 - val_loss: 1.0566 - val_accuracy: 0.7195\n",
        "# Evaluate the model on the test set\n",
        "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
        "print('Test accuracy with SELU:', test_acc)\n",
        "313/313 [==============================] - 1s 3ms/step - loss: 0.9341 - accuracy: 0.6991\n",
        "Test accuracy with SELU: 0.6991000175476074\n",
        "e. Try regularizing the model with alpha dropout. Then, without retraining your model, see if you can achieve better accuracy using MC Dropout.\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Flatten, Dense, Dropout, Conv2D, MaxPooling2D, AlphaDropout\n",
        "from tensorflow.keras.optimizers import Nadam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.initializers import lecun_normal\n",
        "from tensorflow.keras.utils import normalize\n",
        "from tensorflow.keras.layers import Activation\n",
        "import numpy as np\n",
        "# Load the CIFAR10 dataset\n",
        "(X_train, y_train), (X_test, y_test) = keras.datasets.cifar10.load_data()\n",
        "# Normalize the input data\n",
        "X_train = normalize(X_train, axis=1)\n",
        "X_test = normalize(X_test, axis=1)\n",
        "# Define the model architecture with alpha dropout\n",
        "model = Sequential([\n",
        "    Conv2D(32, (3,3), activation='selu', kernel_initializer=lecun_normal(), padding='same', input_shape=(32,32,3)),\n",
        "    Conv2D(32, (3,3), activation='selu', kernel_initializer=lecun_normal(), padding='same'),\n",
        "    MaxPooling2D(pool_size=(2,2)),\n",
        "    AlphaDropout(0.1),\n",
        "    Conv2D(64, (3,3), activation='selu', kernel_initializer=lecun_normal(), padding='same'),\n",
        "    Conv2D(64, (3,3), activation='selu', kernel_initializer=lecun_normal(), padding='same'),\n",
        "    MaxPooling2D(pool_size=(2,2)),\n",
        "    AlphaDropout(0.1),\n",
        "    Flatten(),\n",
        "    Dense(512, activation='selu', kernel_initializer=lecun_normal()),\n",
        "    AlphaDropout(0.5),\n",
        "    Dense(10, activation='softmax')\n",
        "])\n",
        "# Compile the model with Nadam optimizer\n",
        "optimizer = Nadam(lr=0.001)\n",
        "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "WARNING:absl:`lr` is deprecated, please use `learning_rate` instead, or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Nadam.\n",
        "# Define early stopping\n",
        "early_stopping = EarlyStopping(patience=10, restore_best_weights=True)\n",
        "# Train the model with alpha dropout and early stopping\n",
        "history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_split=0.2, callbacks=[early_stopping])\n",
        "Epoch 1/100\n",
        "1250/1250 [==============================] - 11s 7ms/step - loss: 1.8344 - accuracy: 0.3449 - val_loss: 3.5676 - val_accuracy: 0.4931\n",
        "Epoch 2/100\n",
        "1250/1250 [==============================] - 9s 7ms/step - loss: 1.4556 - accuracy: 0.4852 - val_loss: 2.5187 - val_accuracy: 0.6155\n",
        "Epoch 3/100\n",
        "1250/1250 [==============================] - 9s 7ms/step - loss: 1.2903 - accuracy: 0.5487 - val_loss: 2.2874 - val_accuracy: 0.6277\n",
        "Epoch 4/100\n",
        "1250/1250 [==============================] - 9s 7ms/step - loss: 1.1976 - accuracy: 0.5871 - val_loss: 1.9263 - val_accuracy: 0.6386\n",
        "Epoch 5/100\n",
        "1250/1250 [==============================] - 9s 7ms/step - loss: 1.1105 - accuracy: 0.6165 - val_loss: 2.2154 - val_accuracy: 0.6571\n",
        "Epoch 6/100\n",
        "1250/1250 [==============================] - 8s 7ms/step - loss: 1.0512 - accuracy: 0.6373 - val_loss: 1.8176 - val_accuracy: 0.6927\n",
        "Epoch 7/100\n",
        "1250/1250 [==============================] - 9s 7ms/step - loss: 0.9847 - accuracy: 0.6621 - val_loss: 1.6968 - val_accuracy: 0.6824\n",
        "Epoch 8/100\n",
        "1250/1250 [==============================] - 9s 7ms/step - loss: 0.9338 - accuracy: 0.6801 - val_loss: 1.6801 - val_accuracy: 0.7084\n",
        "Epoch 9/100\n",
        "1250/1250 [==============================] - 9s 7ms/step - loss: 0.8758 - accuracy: 0.6995 - val_loss: 1.7509 - val_accuracy: 0.7031\n",
        "Epoch 10/100\n",
        "1250/1250 [==============================] - 9s 7ms/step - loss: 0.8415 - accuracy: 0.7139 - val_loss: 1.9960 - val_accuracy: 0.6789\n",
        "Epoch 11/100\n",
        "1250/1250 [==============================] - 9s 7ms/step - loss: 0.8113 - accuracy: 0.7253 - val_loss: 1.7506 - val_accuracy: 0.7162\n",
        "Epoch 12/100\n",
        "1250/1250 [==============================] - 9s 7ms/step - loss: 0.7768 - accuracy: 0.7365 - val_loss: 2.2384 - val_accuracy: 0.6929\n",
        "Epoch 13/100\n",
        "1250/1250 [==============================] - 9s 7ms/step - loss: 0.7396 - accuracy: 0.7515 - val_loss: 1.6859 - val_accuracy: 0.7063\n",
        "Epoch 14/100\n",
        "1250/1250 [==============================] - 9s 7ms/step - loss: 0.7192 - accuracy: 0.7606 - val_loss: 2.1029 - val_accuracy: 0.7081\n",
        "Epoch 15/100\n",
        "1250/1250 [==============================] - 9s 7ms/step - loss: 0.6941 - accuracy: 0.7679 - val_loss: 2.1383 - val_accuracy: 0.7164\n",
        "Epoch 16/100\n",
        "1250/1250 [==============================] - 9s 7ms/step - loss: 0.6761 - accuracy: 0.7765 - val_loss: 1.9843 - val_accuracy: 0.7136\n",
        "Epoch 17/100\n",
        "1250/1250 [==============================] - 9s 7ms/step - loss: 0.6452 - accuracy: 0.7838 - val_loss: 1.9437 - val_accuracy: 0.7123\n",
        "Epoch 18/100\n",
        "1250/1250 [==============================] - 8s 7ms/step - loss: 0.6335 - accuracy: 0.7930 - val_loss: 1.8890 - val_accuracy: 0.7142\n",
        "# Evaluate the model on the test set\n",
        "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
        "print('Test accuracy with alpha dropout:', test_acc)\n",
        "313/313 [==============================] - 3s 9ms/step - loss: 1.7241 - accuracy: 0.6981\n",
        "Test accuracy with alpha dropout: 0.6980999708175659\n",
        "# Use MC Dropout for improved accuracy without retraining the model\n",
        "n_samples = 100\n",
        "y_probs = np.stack([model.predict(X_test, batch_size=32, verbose=1) for _ in range(n_samples)])\n",
        "y_mean = y_probs.mean(axis=0)\n",
        "y_std = y_probs.std(axis=0)\n",
        "y_pred = np.argmax(y_mean, axis=1)\n",
        "test_acc_mc = (y_pred == y_test.squeeze()).mean()\n",
        "print('Test accuracy with MC Dropout:', test_acc_mc)\n",
        "313/313 [==============================] - 1s 3ms/step\n",
        "313/313 [==============================] - 1s 2ms/step\n",
        "313/313 [==============================] - 1s 2ms/step\n",
        "313/313 [==============================] - 1s 2ms/step\n",
        "313/313 [==============================] - 1s 2ms/step\n",
        "313/313 [==============================] - 1s 2ms/step\n",
        "313/313 [==============================] - 1s 2ms/step\n",
        "313/313 [==============================] - 1s 2ms/step\n",
        "313/313 [==============================] - 1s 3ms/step\n",
        "313/313 [==============================] - 1s 2ms/step\n",
        "313/313 [==============================] - 1s 2ms/step\n",
        "313/313 [==============================] - 1s 2ms/step\n",
        "313/313 [==============================] - 1s 2ms/step\n",
        "313/313 [==============================] - 1s 2ms/step\n",
        "313/313 [==============================] - 1s 2ms/step\n",
        "313/313 [==============================] - 1s 2ms/step\n",
        "313/313 [==============================] - 1s 2ms/step\n",
        "313/313 [==============================] - 1s 4ms/step\n",
        "313/313 [==============================] - 1s 2ms/step\n",
        "313/313 [==============================] - 1s 2ms/step\n",
        "313/313 [==============================] - 1s 2ms/step\n",
        "313/313 [==============================] - 1s 2ms/step\n",
        "313/313 [==============================] - 1s 2ms/step\n",
        "313/313 [==============================] - 1s 2ms/step\n",
        "313/313 [==============================] - 1s 2ms/step\n",
        "313/313 [==============================] - 1s 3ms/step\n",
        "313/313 [==============================] - 1s 3ms/step\n",
        "313/313 [==============================] - 1s 2ms/step\n",
        "313/313 [==============================] - 1s 2ms/step\n",
        "313/313 [==============================] - 1s 2ms/step\n",
        "313/313 [==============================] - 1s 2ms/step\n",
        "313/313 [==============================] - 1s 2ms/step\n",
        "313/313 [==============================] - 1s 2ms/step\n",
        "313/313 [==============================] - 1s 2ms/step\n",
        "313/313 [==============================] - 1s 2ms/step\n",
        "313/313 [==============================] - 1s 3ms/step\n",
        "313/313 [==============================] - 1s 2ms/step\n",
        "313/313 [==============================] - 1s 2ms/step\n",
        "313/313 [==============================] - 1s 2ms/step\n",
        "313/313 [==============================] - 1s 2ms/step\n",
        "313/313 [==============================] - 1s 2ms/step\n",
        "313/313 [==============================] - 1s 2ms/step\n",
        "313/313 [==============================] - 1s 2ms/step\n",
        "313/313 [==============================] - 1s 3ms/step\n",
        "313/313 [==============================] - 1s 3ms/step\n",
        "313/313 [==============================] - 1s 2ms/step\n",
        "313/313 [==============================] - 1s 2ms/step\n",
        "313/313 [==============================] - 1s 2ms/step\n",
        "313/313 [==============================] - 1s 2ms/step\n",
        "313/313 [==============================] - 1s 2ms/step\n",
        "313/313 [==============================] - 1s 2ms/step\n",
        "313/313 [==============================] - 1s 2ms/step\n",
        "313/313 [==============================] - 1s 3ms/step\n",
        "313/313 [==============================] - 1s 2ms/step\n",
        "313/313 [==============================] - 1s 2ms/step\n",
        "313/313 [==============================] - 1s 2ms/step\n",
        "313/313 [==============================] - 1s 2ms/step\n",
        "313/313 [==============================] - 1s 2ms/step\n",
        "313/313 [==============================] - 1s 2ms/step\n",
        "313/313 [==============================] - 1s 3ms/step\n",
        "313/313 [==============================] - 1s 3ms/step\n",
        "313/313 [==============================] - 1s 2ms/step\n",
        "313/313 [==============================] - 1s 2ms/step\n",
        "313/313 [==============================] - 1s 2ms/step\n",
        "313/313 [==============================] - 1s 2ms/step\n",
        "313/313 [==============================] - 1s 2ms/step\n",
        "313/313 [==============================] - 1s 2ms/step\n",
        "313/313 [==============================] - 1s 2ms/step\n",
        "313/313 [==============================] - 1s 3ms/step\n",
        "313/313 [==============================] - 1s 3ms/step\n",
        "313/313 [==============================] - 1s 2ms/step\n",
        "313/313 [==============================] - 1s 2ms/step\n",
        "313/313 [==============================] - 1s 2ms/step\n",
        "313/313 [==============================] - 1s 2ms/step\n",
        "313/313 [==============================] - 1s 2ms/step\n",
        "313/313 [==============================] - 1s 2ms/step\n",
        "313/313 [==============================] - 1s 2ms/step\n",
        "313/313 [==============================] - 1s 2ms/step\n",
        "313/313 [==============================] - 1s 3ms/step\n",
        "313/313 [==============================] - 1s 2ms/step\n",
        "313/313 [==============================] - 1s 2ms/step\n",
        "313/313 [==============================] - 1s 2ms/step\n",
        "313/313 [==============================] - 1s 2ms/step\n",
        "313/313 [==============================] - 1s 2ms/step\n",
        "313/313 [==============================] - 1s 2ms/step\n",
        "313/313 [==============================] - 1s 2ms/step\n",
        "313/313 [==============================] - 1s 3ms/step\n",
        "313/313 [==============================] - 1s 3ms/step\n",
        "313/313 [==============================] - 1s 2ms/step\n",
        "313/313 [==============================] - 1s 2ms/step\n",
        "313/313 [==============================] - 1s 2ms/step\n",
        "313/313 [==============================] - 1s 2ms/step\n",
        "313/313 [==============================] - 1s 2ms/step\n",
        "313/313 [==============================] - 1s 2ms/step\n",
        "313/313 [==============================] - 1s 2ms/step\n",
        "313/313 [==============================] - 1s 3ms/step\n",
        "313/313 [==============================] - 1s 3ms/step\n",
        "313/313 [==============================] - 1s 2ms/step\n",
        "313/313 [==============================] - 1s 2ms/step\n",
        "313/313 [==============================] - 1s 2ms/step\n",
        "Test accuracy with MC Dropout: 0.6981\n",
        "Yes, we can see that we achieved slightly better accuracy with MC Dropout (0.6981) compared to alpha dropout (0.6980) without retraining the model. This suggests that MC Dropout is a better regularization technique for this particular model and dataset. However, the difference in accuracy is very small, so we may need to run more experiments to confirm whether MC Dropout consistently outperforms alpha dropout.\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "-o01_3_Z-EAb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p_CDvxEh94rx"
      },
      "outputs": [],
      "source": []
    }
  ]
}