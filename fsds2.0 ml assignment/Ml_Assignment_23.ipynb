{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. What are the key reasons for reducing the dimensionality of a dataset? What are the major\n",
        "disadvantages?"
      ],
      "metadata": {
        "id": "hH2qLqaC-FWF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dimensionality reduction brings many advantages to your machine learning data, including:\n",
        "\n",
        "Fewer features mean less complexity.\n",
        "\n",
        "You will need less storage space because you have fewer data.\n",
        "Fewer features require less computation time.\n",
        "\n",
        "Model accuracy improves due to less misleading data"
      ],
      "metadata": {
        "id": "DKg5PnT5-FSl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What is the dimensionality curse?"
      ],
      "metadata": {
        "id": "TV7jAO9L-FNG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Curse of Dimensionality describes the explosive nature of increasing data dimensions and its resulting exponential increase in computational efforts required for its processing and/or analysis."
      ],
      "metadata": {
        "id": "_1_nvY4o-FGA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Tell if its possible to reverse the process of reducing the dimensionality of a dataset? If so, how\n",
        "can you go about doing it? If not, what is the reason?"
      ],
      "metadata": {
        "id": "7QThRdEc-h60"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " No, dimensionality reduction is not reversible in general.Once a dataset's dimensionality has been reduced using one of the algorithms we discussed, it is almost always impossible to perfectly reverse the operation, because some information gets lost during dimensionality reduction."
      ],
      "metadata": {
        "id": "36CQWEUL-h2y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Can PCA be utilized to reduce the dimensionality of a nonlinear dataset with a lot of variables?"
      ],
      "metadata": {
        "id": "mFEQlYLR-hz0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, PCA can be used to significantly reduce dimensionality of highly non-linear dataset because it can at least get rid of useless dimensions. If there are no useless dimensions, reducing dimensionality with PCA will lose too much information."
      ],
      "metadata": {
        "id": "D-_orFi_-gK7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Assume you&#39;re running PCA on a 1,000-dimensional dataset with a 95 percent explained variance\n",
        "ratio. What is the number of dimensions that the resulting dataset would have?"
      ],
      "metadata": {
        "id": "O6gbT6dK_EnV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "it depends on dataset."
      ],
      "metadata": {
        "id": "f7IEiM8__oYk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Will you use vanilla PCA, incremental PCA, randomized PCA, or kernel PCA in which situations?"
      ],
      "metadata": {
        "id": "vznOE9II_v4O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vanilla PCA: the dataset fit in memory\n",
        "\n",
        "Incremental PCA: larget dataset that don't fit in memory, online taks\n",
        "\n",
        "Randomized PCA: considerably reduce dimensionality and the dataset fit the memory.\n",
        "\n",
        "kernel PCA: used for nonlinear PCA"
      ],
      "metadata": {
        "id": "TDUBw7sx_v0q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. How do you assess a dimensionality reduction algorithm&#39;s success on your dataset?"
      ],
      "metadata": {
        "id": "rRQFF29vAFkf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Measure the reconstruction error\n",
        "\n",
        "2. Measure the performance in second Machine Learning algorithm"
      ],
      "metadata": {
        "id": "8t5Q8mBBAMHv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Is it logical to use two different dimensionality reduction algorithms in a chain?"
      ],
      "metadata": {
        "id": "ITueyZtGAUUh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes,it can absolutely make sense to chain two different dimensionality reduction algorithms. A common example is using PCA to quickly get rid of a large number of useless dimensions, then applying another much slower dimensionality reduction algorithm, such as LLE."
      ],
      "metadata": {
        "id": "69GmIvDVAl_j"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sB4rChOJ9yco"
      },
      "outputs": [],
      "source": []
    }
  ]
}